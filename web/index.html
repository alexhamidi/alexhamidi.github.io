<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../favicon/favicon-16x16.png">
  <link rel="icon" href="../favicon/favicon.ico">
  <link rel="manifest" href="../favicon/site.webmanifest">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <script src="https://cdn.tailwindcss.com"></script>
  <title>Building Umari - Alex Hamidi</title>
</head>

<body class="flex flex-col items-center text-gray-900 font-mono overflow-x-hidden bg-[#fcfcfa] color-[#121212]">

  <div class="flex flex-col gap-8 max-w-[700px] px-[20px] mb-12">

    <div class="flex items-center justify-between mt-8 mb-4">
      <a href="/" class="text-sm text-gray-600 hover:text-gray-900 underline">← back to home</a>
      <div class="text-xs text-gray-500">October 2025</div>
    </div>

    <article class="blog-content text-sm">
      <h1 class="text-3xl font-bold mb-6">Building Umari</h1>

      <p class="mb-4 leading-relaxed">I've been working on Umari, a web agent that lets you control your browser through natural language. The idea is simple: you type what you want done, and it figures out the clicks and keystrokes. But getting it to actually work fast enough to be useful turned into an interesting engineering problem.</p>

      <p class="mb-4 leading-relaxed">Umari runs as a lightweight interface where you describe tasks in plain English. It takes screenshots to see what's on screen, uses reasoning to plan the next steps, then executes them. The challenge was making this fast enough that it doesn't feel like you're waiting on the computer.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Why GPT-5</h3>

      <p class="mb-4 leading-relaxed"><strong>I built Umari entirely with GPT-5's reasoning capabilities.</strong> Used it for development, debugging, training data generation—basically everything. The high-compute reasoning mode helped plan multi-step interactions, and GPT-5 Vision handled screen understanding. Previous models couldn't reliably maintain context across long workflows.</p>

      <p class="mb-4 leading-relaxed">What makes GPT-5 work here is how it handles complex chains of actions without losing track. You can tell it "start playing this game" and it'll break that down into discrete steps while adapting when the UI changes unexpectedly. That context retention is crucial.</p>

      <p class="mb-4 leading-relaxed"><strong>I tuned compute allocation based on task complexity.</strong> Simple clicks get minimal reasoning, complex workflows get more. For ambiguous UIs, high reasoning effort maps out interaction sequences with error handling. This tradeoff between speed and reliability makes the difference between something that works and something people actually use.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Architecture</h3>

      <p class="mb-4 leading-relaxed"><strong>The system uses two models: GPT-5 for reasoning about what to do, and a small grounding model for finding exactly where to click.</strong> This split makes sense because deciding what action to take is fundamentally different from executing it precisely.</p>

      <p class="mb-4 leading-relaxed">GPT-5 sees the screen and outputs a semantic action like "click the blue Submit button." Then the grounding model takes that description, looks at the screenshot, and outputs exact pixel coordinates. One handles the "what," the other handles the "where."</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Why vision tokens are expensive</h3>

      <p class="mb-4 leading-relaxed">Vision models process screenshots as tiles. A 1920×1080 screen becomes 6 tiles at 170 tokens each, plus reasoning tokens.</p>

      <div class="bg-gray-50 border border-gray-200 rounded-lg p-4 my-4 text-sm">
        <div class="mb-2"><span class="text-gray-600">Per step:</span> <strong>3,200–9,400 tokens</strong></div>
        <div class="mb-2"><span class="text-gray-600">100 steps:</span> <strong>$3.20–$9.40</strong></div>
        <div class="mb-2"><span class="text-gray-600">With caching:</span> <strong>$0.32–$0.94</strong> <span class="text-xs text-gray-500">(90% discount)</span></div>
        <div class="pt-2 border-t border-gray-300 mt-2"><span class="text-gray-600">Latency:</span> <strong>2–5 seconds per action</strong> <span class="text-xs text-gray-500">(100-step task = 3–8 minutes)</span></div>
      </div>

      <p class="mb-4 leading-relaxed">Running workflows at scale gets expensive fast. Without optimization, costs compound quickly—$28k/month for repeated tasks. And latency adds up: what takes a human 50 minutes could take 5–13 hours in compute time.</p>

      <p class="mb-4 leading-relaxed">My approach: split reasoning from grounding, then cache aggressively. UI elements don't move much between actions, so a Submit button at (834, 672) stays cached. Combined with a lightweight saliency scorer that identifies interactive regions, this gets 70%+ cache hits and 10–50ms grounding latency.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Training</h3>

      <p class="mb-4 leading-relaxed"><strong>I trained the grounding model with GRPO (Group Relative Policy Optimization). Rewards are binary: 1 if the click hits the target element, 0 otherwise.</strong> This works well with patches—small enough that any click within the element gets rewarded.</p>

      <p class="mb-4 leading-relaxed">For training data, I used trajectory augmentation on recorded workflows. One demonstration becomes multiple trajectories by varying timing and UI states. This makes the model more robust across different scenarios without needing tons of manual labeling.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Making it fast</h3>

      <p class="mb-4 leading-relaxed">Test-time compute gets a lot of hype, but for web agents the bottleneck is usually grounding, not reasoning. A 7B model can handle most click targeting if you route properly.</p>

      <p class="mb-4 leading-relaxed">For simple UIs, the grounding model runs alone—no planner call needed. This hits ~50ms per action on an A100. I only escalate to the reasoning model when there's ambiguity: high saliency entropy, multiple similar targets, recent errors, or duplicate UI elements.</p>

      <p class="mb-4 leading-relaxed">The tradeoff is straightforward: speed for simple tasks, accuracy for complex ones. Same models, different routing based on confidence.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">What's next</h3>

      <p class="mb-4 leading-relaxed">I'm working on moving to a streaming pipeline—consuming frames at 20-30 fps, emitting actions at 5-10 Hz. This would close the perception-action loop properly and handle drag/hover/scroll more naturally.</p>

      <p class="mb-4 leading-relaxed">Also planning to compile repeated workflows into micro-policies. If you're running the same RPA task daily, the model should learn it and execute locally without calling the planner. The reasoning model becomes a teacher, not a dependency.</p>

      <p class="mb-4 leading-relaxed">Long term, the goal is end-to-end. Tesla went from rules-based driving to cameras→steering directly. Same principle here: screenshots→actions, minimal intermediate abstractions.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Related Work</h3>

      <div class="text-xs text-gray-600 leading-relaxed">
        <p class="mb-2">He, Y., Jin, J., & Liu, P. (2025). <a href="https://gair-nlp.github.io/PC-Agent-E/" target="_blank" rel="noopener noreferrer" class="underline text-gray-700 hover:text-gray-900">Efficient Agent Training for Computer Use</a>. <em>arXiv preprint arXiv:2505.13909</em>.</p>
        <p class="mb-2">Yang, Y., Li, D., Dai, Y., Yang, Y., Luo, Z., Zhao, Z., Hu, Z., Huang, J., Saha, A., Chen, Z., Xu, R., Pan, L., Xiong, C., & Li, J. (2025). <a href="https://arxiv.org/abs/2507.05791" target="_blank" rel="noopener noreferrer" class="underline text-gray-700 hover:text-gray-900">GTA1: GUI Test-time Scaling Agent</a>. <em>arXiv preprint arXiv:2507.05791</em>.</p>
      </div>

    </article>

    <div class="flex items-center justify-between mt-8 pt-8 border-t border-gray-200">
      <div class="text-sm text-gray-600">
        Written by <a href="/" class="underline hover:text-gray-900">Alex Hamidi</a>
      </div>
      <div class="flex gap-4 text-sm">
        <a href="https://twitter.com/ahamidi_" class="text-gray-600 hover:text-gray-900 underline">Twitter</a>
        <a href="https://github.com/alexhamidi/" class="text-gray-600 hover:text-gray-900 underline">GitHub</a>
      </div>
    </div>

  </div>

</body>

</html>