<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="../favicon/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../favicon/favicon-16x16.png">
  <link rel="icon" href="../favicon/favicon.ico">
  <link rel="manifest" href="../favicon/site.webmanifest">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <script src="https://cdn.tailwindcss.com"></script>
  <title>Teaching GPT-5 to Use a Computer - Alex Hamidi</title>
</head>

<body class="flex flex-col items-center text-gray-900 font-mono overflow-x-hidden bg-[#fcfcfa] color-[#121212]">

  <div class="flex flex-col gap-8 max-w-[700px] px-[20px] mb-12">

    <div class="flex items-center justify-between mt-8 mb-4">
      <a href="/" class="text-sm text-gray-600 hover:text-gray-900 underline">← back to home</a>
      <div class="text-xs text-gray-500">August 2025</div>
    </div>

    <article class="blog-content text-sm">
      <h1 class="text-3xl font-bold mb-6">Teaching GPT-5 to Use a Computer</h1>

      <p class="mb-4 leading-relaxed">Over the weekend, <a href="https://x.com/OpenAIDevs/status/1955774997735333936" target="_blank" rel="noopener noreferrer" class="underline text-gray-900 transition-all duration-500 hover:text-gray-50 hover:bg-gray-700 px-0.5">I won #3 at OpenAI's GPT-5 Hackathon</a> with Archon - a copilot for your computer. It comes with a mini vision model for speed, and GPT-5 for variable reasoning to plan. I took some time to write about how it works, and our approach to building a self-driving computer with inference math, and the tradeoffs we made.</p>

      <p class="mb-4 leading-relaxed">Archon is a small bar that sits at the bottom of your Mac/Windows screen where you can type what you want your computer to do in natural language. It takes screenshots to see what's on screen, uses GPT-5's reasoning to plan, then a custom fine-tuned model executes clicks and keystrokes.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">GPT-5: Why it worked for us</h3>

      <p class="mb-4 leading-relaxed"><strong>Archon was built entirely using GPT-5's advanced reasoning capabilities.</strong> We leveraged probably every aspect of GPT-5 from initial development to debugging to training. Codex CLI with GPT-5 with High Thinking enabled us to build the entire app, and GPT-5 with Vision enabled us to see and perceive the screen. GPT-5's reasoning ability was crucial for instruction following, and planning. This quite simply wasn't possible with any other model.</p>

      <p class="mb-4 leading-relaxed">What makes GPT-5 particularly suited for computer control is its ability to <em>reason through complex multi-step processes</em> while maintaining context across long interactions. Unlike previous models that might hallucinate or lose track of the current state, GPT-5's chain-of-thought reasoning allows it to break down "start playing this game" into discrete, executable steps while adapting to unexpected UI changes.</p>

      <p class="mb-4 leading-relaxed"><strong>We calibrated how much compute to use strategically to trade off accuracy and latency.</strong> For complex workflows, high reasoning effort mapped out interaction sequences with error handling. GPT-5-mini with function calling preambles enabled us to show the user what we were thinking while simultaneously calling our grounding model. This adaptive approach keeps the user in mind.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">How it actually works</h3>

      <p class="mb-4 leading-relaxed"><strong>Archon uses a hierarchical approach: a large reasoning model (GPT-5/o-series) decides what to do, and prava-fc-small (Prava's Fast Click grounding model) figures out exactly where to click.</strong> This split matters because reasoning and grounding are fundamentally different problems with different computational requirements.</p>

      <p class="mb-4 leading-relaxed">The reasoning model sees the screen and your request, then outputs a semantic action: "click the blue Submit button at the bottom." Descriptions enable reasoning to be done in natural language. <strong>prava-fc-small</strong> takes that description plus the screenshot and outputs exact pixel coordinates: (523, 412). One model for the "what," another for the "where."</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Why vision tokens are expensive (and how we optimize them)</h3>

      <p class="mb-4 leading-relaxed">For GPT-5's computer-using agent, each action involves vision, reasoning, and response. A 1920×1080 screenshot becomes 6 tiles at 170 tokens each, plus reasoning tokens billed as output.</p>

      <div class="bg-gray-50 border border-gray-200 rounded-lg p-4 my-4 text-sm">
        <div class="mb-2"><span class="text-gray-600">Per step:</span> <strong>3,200–9,400 tokens</strong></div>
        <div class="mb-2"><span class="text-gray-600">100 steps:</span> <strong>$3.20–$9.40</strong></div>
        <div class="mb-2"><span class="text-gray-600">With caching:</span> <strong>$0.32–$0.94</strong> <span class="text-xs text-gray-500">(90% discount)</span></div>
        <div class="pt-2 border-t border-gray-300 mt-2"><span class="text-gray-600">Latency:</span> <strong>2–5 seconds per action</strong> <span class="text-xs text-gray-500">(100-step task = 3–8 minutes)</span></div>
      </div>

      <p class="mb-4 leading-relaxed">Running the same workflow 100 times daily costs $940, over $28,000/month without caching. Each run takes 3–8 minutes, so what would take a human 50 minutes would take 5–13 hours of compute time.</p>

      <p class="mb-4 leading-relaxed">Our approach: split reasoning from grounding. GPT-5 decides "click the blue Submit button," prava-fc-small finds the exact coordinates. We cache patches aggressively since UI elements rarely move between actions, so the Submit button at (834, 672) stays cached across clicks. Combined with a 3MB saliency scorer that identifies interactive regions, we achieve 70%+ cache hits and 10–50ms grounding latency.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Training: GRPO and synthetic data generation</h3>

      <p class="mb-4 leading-relaxed"><strong>We trained prava-fc-small with GRPO (Group Relative Policy Optimization), where rewards are binary: 1 if the click lands inside the target UI element, 0 otherwise.</strong> Patches work well for this because they're small enough that clicking anywhere within a patch-covered element still gets rewarded.</p>

      <p class="mb-4 leading-relaxed">To scale training data, we used trajectory augmentation on human demonstrations. From one recorded workflow, we generate multiple related trajectories by varying timing, UI states, and interaction patterns - effectively "boosting" the grounding model's robustness across different scenarios.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Speed: adaptive compute that feels instant</h3>

      <p class="mb-4 leading-relaxed">Test-time compute is getting extremely hyped these days, particularly off of the success of the o-series models. Good thing for prava-fc-small is that it's a lot of "grounding work" and not a lot of "knowledge work". You can get a lot of mileage out of a 7B model if you instead vary the reasoning and determine how to properly pipeline the tasks.</p>

      <p class="mb-4 leading-relaxed">On this path, <strong>prava-fc-small</strong> runs alone (no planner call), hitting ~50 ms per action on a A100. The router only escalates when signals are uncertain: high saliency entropy, too many candidate targets, recent misclicks, or ambiguous copy (e.g., multiple "Submit" buttons).</p>

      <p class="mb-4 leading-relaxed">The fundamental tradeoff is simple: consumers want one thing done fast, enterprises want many things done efficiently. Same model, different routing strategy.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">What's next: streaming control and unifying the stack</h3>

      <p class="mb-4 leading-relaxed">In the future we hope to run a streaming capture pipeline similar to Gemma 3. Consuming frames at 20-30 fps, emitting actions at 5-10 Hz, and verifying state on each commit. This closes the perception-action loop for drag/hover/scroll and makes motion feel natural.</p>

      <p class="mb-4 leading-relaxed">We also plan to <em>compile</em> solved steps into micro-policies. If you're running something like a RPA task or similar workflow as before, you can simply run the execution locally (with prava-fc-small running locally) and not have to worry about the planning. Over time, the planner is a background teacher, not a crutch.</p>

      <p class="mb-4 leading-relaxed">We will distill those plans into the local model so more steps stay on the fast path. The path forward is to adopt an end-to-end approach to the problem. For Tesla that's camera, steering, acceleration. For us it's screen, mouse, keyboard.</p>

      <h3 class="text-xl font-semibold mt-8 mb-4">Related Work</h3>

      <div class="text-xs text-gray-600 leading-relaxed">
        <p class="mb-2">He, Y., Jin, J., & Liu, P. (2025). <a href="https://gair-nlp.github.io/PC-Agent-E/" target="_blank" rel="noopener noreferrer" class="underline text-gray-700 hover:text-gray-900">Efficient Agent Training for Computer Use</a>. <em>arXiv preprint arXiv:2505.13909</em>.</p>
        <p class="mb-2">Yang, Y., Li, D., Dai, Y., Yang, Y., Luo, Z., Zhao, Z., Hu, Z., Huang, J., Saha, A., Chen, Z., Xu, R., Pan, L., Xiong, C., & Li, J. (2025). <a href="https://arxiv.org/abs/2507.05791" target="_blank" rel="noopener noreferrer" class="underline text-gray-700 hover:text-gray-900">GTA1: GUI Test-time Scaling Agent</a>. <em>arXiv preprint arXiv:2507.05791</em>.</p>
      </div>

    </article>

    <div class="flex items-center justify-between mt-8 pt-8 border-t border-gray-200">
      <div class="text-sm text-gray-600">
        Written by <a href="/" class="underline hover:text-gray-900">Alex Hamidi</a>
      </div>
      <div class="flex gap-4 text-sm">
        <a href="https://twitter.com/ahamidi_" class="text-gray-600 hover:text-gray-900 underline">Twitter</a>
        <a href="https://github.com/alexhamidi/" class="text-gray-600 hover:text-gray-900 underline">GitHub</a>
      </div>
    </div>

  </div>

</body>

</html>